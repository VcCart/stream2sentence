# stream2sentence.py
# -*- coding: utf-8 -*-
"""
Real-time processing and delivery of sentences from a continuous stream of characters or text chunks.
(–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–∏—Ö–æ–≤, –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞, –ø–ª–∞–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, —Ç–æ—á–Ω—ã–µ –ø–∞—É–∑—ã)
"""

import functools
import logging
import re
from typing import (
    AsyncIterable,
    AsyncIterator,
    Awaitable,
    Callable,
    Concatenate,
    Iterable,
    Iterator,
    ParamSpec,
    List,
    Optional,
    Set,
    Tuple
)

import emoji

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ---
current_tokenizer = "stanza"
stanza_initialized = False
nltk_initialized = False
nlp = None # –î–ª—è Stanza

# --- –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è ---
# –°–∏–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ - –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –æ–∑–Ω–∞—á–∞—é—Ç –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–¥–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞)
STRONG_DELIMITERS: Set[str] = {'.', '!', '?', '\n', '\u2026', '\u3002'}  # üá∑üá∫ —É–±—Ä–∞–Ω–æ —Ç–∏—Ä–µ –∏–∑ —Å–∏–ª—å–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π ‚Äî –Ω–µ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
# –°–ª–∞–±—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ - –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–∂–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–∫–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞)
WEAK_DELIMITERS: Set[str] = {',', ';', ':', '‚Äî'}  # üá∑üá∫ —Ç–∏—Ä–µ —Ç–µ–ø–µ—Ä—å —Å–ª–∞–±—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å (–∫–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –≤–Ω—É—Ç—Ä–∏ —Ñ—Ä–∞–∑—ã)
# –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ —Å–∏–º–≤–æ–ª—ã - –Ω–∞ –Ω–∏—Ö –ø–∞—É–∑—É –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ
IGNORED_DELIMITERS: Set[str] = {'"', "'", '(', ')', '[', ']', '{', '}', '¬´', '¬ª', '\r', ' '}  # üá∑üá∫ –¥–µ—Ñ–∏—Å '-' —Ç–µ–ø–µ—Ä—å –Ω–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è
# –í—Å–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ (–¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Å–ª–æ–≤ –∏ –ø–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏)
SENTENCE_FRAGMENT_DELIMITERS: Set[str] = STRONG_DELIMITERS | WEAK_DELIMITERS

# --- –§—É–Ω–∫—Ü–∏–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ ---

def initialize_nltk(debug: bool = False) -> None:
    """
    Initializes NLTK by downloading required data for sentence tokenization.
    """
    global nltk_initialized
    if nltk_initialized:
        return

    logging.info("Initializing NLTK Tokenizer")

    try:
        import nltk
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö, –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        try:
            _ = nltk.data.find('tokenizers/punkt_tab')  # –û–±–Ω–æ–≤–ª–µ–Ω–æ: punkt_tab –≤–º–µ—Å—Ç–æ punkt
        except LookupError:
            nltk.download('punkt_tab', quiet=not debug)  # –û–±–Ω–æ–≤–ª–µ–Ω–æ: punkt_tab
        nltk_initialized = True
    except Exception as e:
        print(f"Error initializing nltk tokenizer: {e}")
        nltk_initialized = False

def initialize_stanza(language: str = "ru", offline: bool = False) -> None:
    """
    Initializes Stanza by downloading required data for sentence tokenization.
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è XTTS2 - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã.
    """
    global nlp, stanza_initialized
    if stanza_initialized:
        return

    logging.info("Initializing Stanza Tokenizer")

    try:
        import stanza
        if not offline:
            stanza.download(language)
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è XTTS2 - —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        nlp = stanza.Pipeline(language, processors='tokenize', download_method=None)
        stanza_initialized = True
    except Exception as e:
        print(f"Error initializing stanza tokenizer: {e}")
        stanza_initialized = False

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---

def _remove_emojis(text: str) -> str:
    """
    Removes emojis from the input text.
    """
    return emoji.replace_emoji(text, "")

async def _generate_characters(
    generator: AsyncIterable[str], 
    log_characters: bool = False
) -> AsyncIterator[str]:
    """
    Generates individual characters from a text generator.
    """
    if log_characters:
        print("Stream: ", end="", flush=True)
    async for chunk in generator:
        for char in chunk:
            if log_characters:
                print(char, end="", flush=True)
            yield char
    if log_characters:
        print()

def _clean_text(
    text: str,
    cleanup_text_emojis: bool = False,
    strip_text: bool = True,
) -> str:
    """
    Cleans the text by removing emojis only.
    """
    if cleanup_text_emojis:
        text = _remove_emojis(text)
    if strip_text:
        text = text.strip()
    return text

def _is_likely_sentence_boundary(text: str, delimiter_pos: int, next_chars: str = "") -> bool:
    """
    –£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ª–∏ —ç—Ç–æ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç False –¥–ª—è –∑–∞–ø—è—Ç—ã—Ö –≤ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤–∞—Ö –∏ —Ç–æ—á–µ–∫ –≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞—Ö.
    """
    if delimiter_pos < 1:
        return False
    
    delimiter_char = text[delimiter_pos]
    
    # –î–ª—è –ó–ê–ü–Ø–¢–´–• - –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—è—Ç–æ–π
    if delimiter_char == ',':
        fragment_before = text[:delimiter_pos].strip()
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—è—Ç–æ–π –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç (–º–µ–Ω—å—à–µ 4 —Å–∏–º–≤–æ–ª–æ–≤) - —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —ç—Ç–æ –Ω–µ –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if len(fragment_before) < 4:
            return False
        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—è—Ç–æ–π –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–ª–æ–≤–æ (2-3 —Å–∏–º–≤–æ–ª–∞) - —Ç–æ–∂–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        words = fragment_before.split()
        if words and len(words[-1]) <= 3:
            return False
    
    # –î–ª—è –¢–û–ß–ï–ö - –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
    elif delimiter_char == '.':
        # –ò—â–µ–º –Ω–∞—á–∞–ª–æ "—Å–ª–æ–≤–∞" –ø–µ—Ä–µ–¥ —Ç–æ—á–∫–æ–π
        word_start = delimiter_pos - 1
        while word_start >= 0 and (text[word_start].isalpha() or text[word_start] in {'-', '_'}):
            word_start -= 1
        
        word_before_dot = text[word_start + 1:delimiter_pos]
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã:
        # - –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–æ–µ "—Å–ª–æ–≤–æ" –ø–µ—Ä–µ–¥ —Ç–æ—á–∫–æ–π (1-2 —Å–∏–º–≤–æ–ª–∞)
        # - –°–ª–µ–¥—É—é—â–∏–π —Å–∏–º–≤–æ–ª - –Ω–µ –ø—Ä–æ–±–µ–ª –∏–ª–∏ —Ç–æ–∂–µ –±—É–∫–≤–∞/—Ç–æ—á–∫–∞
        if (len(word_before_dot) <= 2 and 
            (delimiter_pos + 1 < len(text)) and 
            (text[delimiter_pos + 1].isalnum() or text[delimiter_pos + 1] == '.')):
            return False  # –≠—Ç–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞, –Ω–µ –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    
    return True

def _tokenize_sentences(text: str, tokenize_sentences: Optional[Callable[[str], List[str]]] = None, 
                       poetic_mode: bool = False, preserve_line_breaks: bool = False) -> List[str]:
    """
    Tokenizes sentences from the input text with proper error handling.
    """
    global nlp, current_tokenizer
    try:
        if tokenize_sentences:
            sentences = tokenize_sentences(text)
        else:
            # –î–ª—è –ø–æ—ç—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞ - —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            if poetic_mode:
                # –í –ø–æ—ç—Ç–∏—á–µ—Å–∫–æ–º —Ä–µ–∂–∏–º–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–æ–∫
                if preserve_line_breaks:
                    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º, –Ω–æ –Ω–µ —Å—á–∏—Ç–∞–µ–º –∏—Ö –∫–æ–Ω—Ü–∞–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                    lines = text.split('\n')
                    sentences = [line for line in lines if line.strip()]
                else:
                    # –û–±—ã—á–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –Ω–æ —Å –º–µ–Ω—å—à–µ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –∫ \n
                    text = text.replace('\n', ' ')  # –ó–∞–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
                    if current_tokenizer == "nltk":
                        import nltk
                        sentences = nltk.tokenize.sent_tokenize(text, language='russian')
                    elif current_tokenizer == "stanza":
                        import stanza
                        if nlp is None:
                            logging.warning("Stanza tokenizer not initialized, falling back to raw text.")
                            return [text]
                        doc = nlp(text)
                        sentences = [sentence.text for sentence in doc.sentences]
                    else:
                        raise ValueError(f"Unknown tokenizer: {current_tokenizer}")
            else:
                # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º
                if current_tokenizer == "nltk":
                    import nltk
                    sentences = nltk.tokenize.sent_tokenize(text, language='russian')
                elif current_tokenizer == "stanza":
                    import stanza
                    if nlp is None:
                        logging.warning("Stanza tokenizer not initialized, falling back to raw text.")
                        return [text]
                    doc = nlp(text)
                    sentences = [sentence.text for sentence in doc.sentences]
                else:
                    raise ValueError(f"Unknown tokenizer: {current_tokenizer}")
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–Ω–æ–≤–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ)
        combined_sentences = []
        temp_sentence = ""

        for sentence in sentences:
            if len(sentence) < 10:  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                temp_sentence += sentence + " "
            else:
                if temp_sentence:
                    temp_sentence += sentence
                    combined_sentences.append(temp_sentence.strip())
                    temp_sentence = ""
                else:
                    combined_sentences.append(sentence.strip())

        # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        if temp_sentence:
            combined_sentences.append(temp_sentence.strip())

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        result = combined_sentences if combined_sentences and len(combined_sentences) < len(sentences) else sentences
        result = result if result else [text]
        return result
    except Exception as e:
        logging.warning(f"Tokenization error: {e}, returning raw text")
        return [text]

def init_tokenizer(tokenizer: str, language: str = "ru", offline: bool = False, debug: bool = False) -> None:
    """
    Initializes the sentence tokenizer.
    """
    global current_tokenizer
    if tokenizer == "nltk":
        initialize_nltk(debug)
    elif tokenizer == "stanza":
        initialize_stanza(language, offline=offline)
    else:
        logging.warning(f"Unknown tokenizer: {tokenizer}")

# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ---

async def generate_sentences_async(
    generator: AsyncIterable[str],  
    context_size: int = 15,
    context_size_look_overhead: int = 8,
    minimum_sentence_length: int = 12,  # üá∑üá∫ –£–í–ï–õ–ò–ß–ï–ù–û –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
    minimum_first_fragment_length: int = 8,  # üá∑üá∫ –£–ú–ï–ù–¨–®–ï–ù–û –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
    quick_yield_single_sentence_fragment: bool = True,
    quick_yield_for_all_sentences: bool = False,
    quick_yield_every_fragment: bool = False,
    cleanup_text_emojis: bool = False,
    tokenize_sentences: Optional[Callable[[str], List[str]]] = None,
    tokenizer: str = "stanza",
    language: str = "ru",
    log_characters: bool = False,
    filter_first_non_alnum_characters: bool = True,
    force_first_fragment_after_words: int = 15,  # –£–≤–µ–ª–∏—á–µ–Ω
    debug: bool = False,
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏
    poetic_mode: bool = False,
    preserve_line_breaks: bool = False,
    adaptive_context: bool = True,
    smooth_short_fragments: bool = True,
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è XTTS2
    max_buffer_size: int = 700,
    tokenization_interval: int = 6,  # –£–≤–µ–ª–∏—á–µ–Ω –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è —á–∞—Å—Ç—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
    strict_punctuation_mode: bool = True,
    min_chars_after_delimiter: int = 25,  # üá∑üá∫ –£–í–ï–õ–ò–ß–ï–ù–û –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    # –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–ø–æ–ª–µ–∑–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è)
    sentence_fragment_delimiters: str = ".?!;:,\n‚Ä¶)]}„ÄÇ‚Äî",  # –ö–∞—Å—Ç–æ–º–∏–∑–∏—Ä—É–µ–º—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
    full_sentence_delimiters: str = ".?!\n‚Ä¶„ÄÇ‚Äî",  # –ö–∞—Å—Ç–æ–º–∏–∑–∏—Ä—É–µ–º—ã–µ –ø–æ–ª–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
    # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    cleanup_text_links: bool = False,
) -> AsyncIterator[str]:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è XTTS2: –ø–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–∏—Ö–æ–≤ –∏ –ø—Ä–æ–∑—ã, –ø–ª–∞–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞.
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è Stanza –∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –ü–∞—É–∑—ã —Ç–æ–ª—å–∫–æ –Ω–∞ –∑–Ω–∞–∫–∞—Ö –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è.
    """

    global current_tokenizer
    current_tokenizer = tokenizer
    init_tokenizer(current_tokenizer, language, debug)

    buffer = ""
    is_first_sentence = True
    word_count = 0
    last_delimiter_position = -1
    fragment_count = 0
    tokenization_counter = 0
    chars_since_last_strong_delim = 0
    # üá∑üá∫ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞—É–∑ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–Ω–∞–∫–∞ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    weak_pause_ratio = 0.6  # üá∑üá∫ –∫–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç—ã—Ö, —Ç–æ—á–µ–∫ —Å –∑–∞–ø—è—Ç–æ–π, —Ç–∏—Ä–µ
    strong_pause_ratio = 1.0  # üá∑üá∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø–∞—É–∑–∞ –ø–æ—Å–ª–µ —Ç–æ—á–∫–∏, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–Ω–∞–∫–∞

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–æ–≤—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –≤ sets –¥–ª—è –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    sentence_fragment_delims_set = set(sentence_fragment_delimiters)
    full_sentence_delims_set = set(full_sentence_delimiters)

    # Adjust quick yield flags based on settings
    if quick_yield_every_fragment:
        quick_yield_for_all_sentences = True

    if quick_yield_for_all_sentences:
        quick_yield_single_sentence_fragment = True

    async for char in _generate_characters(generator, log_characters):

        if char:
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞—á–∞–ª—å–Ω—ã—Ö –Ω–µ-–∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            if len(buffer) == 0 and filter_first_non_alnum_characters:
                if not char.isalnum() and char not in {'(', '[', '¬´', '"', "'"}:
                    continue

            buffer += char
            buffer = buffer.lstrip()

            # Update word count and track delimiter distances
            if char.isspace() or char in sentence_fragment_delims_set:
                word_count += 1
                if char in full_sentence_delims_set:
                    chars_since_last_strong_delim = 0
                elif char not in IGNORED_DELIMITERS:
                    chars_since_last_strong_delim += 1
            else:
                chars_since_last_strong_delim += 1

            if debug:
                print(f"\033[36mDebug: Buffer size: {len(buffer)}, Words: {word_count}\033[0m")

            # --- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±—É—Ñ–µ—Ä–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ ---
            if len(buffer) > max_buffer_size:
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–¥–∞—Ç—å —á–∞—Å—Ç—å –±—É—Ñ–µ—Ä–∞
                forced_text = buffer[:max_buffer_size//2]
                buffer = buffer[max_buffer_size//2:]
                yield_text = _clean_text(forced_text, cleanup_text_emojis)
                yield yield_text
                word_count = max(0, word_count - 6)
                tokenization_counter = 0
                chars_since_last_strong_delim = 0
                continue

            # --- –õ–æ–≥–∏–∫–∞ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ ---
            if (
                is_first_sentence
                and len(buffer) >= minimum_first_fragment_length
                and quick_yield_single_sentence_fragment
            ):
                # –£–ª—É—á—à–µ–Ω–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
                if (
                    buffer[-1] in sentence_fragment_delims_set
                    or char.isspace() and word_count >= force_first_fragment_after_words
                ):
                    # üá∑üá∫ –§–ò–õ–¨–¢–†: –Ω–µ –≤—ã–¥–∞–≤–∞—Ç—å –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –∑–∞–ø—è—Ç—ã–º–∏
                    if (buffer[-1] == ',' and len(buffer.strip()) < 6):
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –∑–∞–ø—è—Ç—ã–º–∏
                    
                    if debug:
                        if buffer[-1] in sentence_fragment_delims_set:
                            print(f"\033[36mDebug: Yielding first fragment: \"{buffer}\" (delimiter)\033[0m")
                        else:
                            print(f"\033[36mDebug: Yielding first fragment: \"{buffer}\" (word limit)\033[0m")

                    yield_text = _clean_text(buffer, cleanup_text_emojis)
                    yield yield_text

                    buffer = ""
                    if not quick_yield_every_fragment:
                        is_first_sentence = False

                    word_count = 0
                    fragment_count += 1
                    tokenization_counter = 0
                    chars_since_last_strong_delim = 0
                    continue

            # Continue accumulating if buffer is too small
            if len(buffer) <= minimum_sentence_length + context_size:
                continue

            # Update last delimiter position if a new STRONG delimiter is found
            if char in full_sentence_delims_set:
                last_delimiter_position = len(buffer) - 1

            # --- –£–õ–£–ß–®–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò ---
            if len(buffer) > context_size:
                delimiter_char = buffer[-context_size]
                
                # –í —Å—Ç—Ä–æ–≥–æ–º —Ä–µ–∂–∏–º–µ –ø–∞—É–∑—ã —Ç–æ–ª—å–∫–æ –Ω–∞ –∑–Ω–∞–∫–∞—Ö –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
                if strict_punctuation_mode:
                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–µ-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                    if delimiter_char not in sentence_fragment_delims_set:
                        continue
                else:
                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–∏–º–≤–æ–ª—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø–∞—É–∑—É –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ
                    if delimiter_char in IGNORED_DELIMITERS:
                        continue
                
                # –¢–æ–ª—å–∫–æ –¥–ª—è —Å–∏–ª—å–Ω—ã—Ö –∏ —Å–ª–∞–±—ã—Ö —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
                is_strong_delim = delimiter_char in full_sentence_delims_set
                is_weak_delim = delimiter_char in set(";:,")  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ —Å–ª–∞–±—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
                
                if is_strong_delim or is_weak_delim:
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ø–æ—Å–ª–µ –∑–Ω–∞–∫–∞ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞
                    chars_after_delimiter = context_size - 1
                    if chars_after_delimiter < min_chars_after_delimiter:
                        continue  # –°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –∑–Ω–∞–∫–∞ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
                    
                    # üá∑üá∫ –£–ú–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –î–õ–Ø –†–£–°–°–ö–û–ì–û –Ø–ó–´–ö–ê: —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ª–æ–∂–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã
                    if not _is_likely_sentence_boundary(buffer, len(buffer) - context_size):
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º - —ç—Ç–æ –Ω–µ –Ω–∞—Å—Ç–æ—è—â–∞—è –≥—Ä–∞–Ω–∏—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    tokenization_counter += 1
                    # –î–ª—è —Å–ª–∞–±—ã—Ö —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–µ, –¥–ª—è —Å–∏–ª—å–Ω—ã—Ö - —á–∞—â–µ
                    if is_weak_delim:
                        check_interval = max(1, int(tokenization_interval * weak_pause_ratio))
                    elif is_strong_delim:
                        check_interval = int(tokenization_interval * strong_pause_ratio)
                    else:
                        check_interval = tokenization_interval
                    if tokenization_counter < check_interval:
                        continue
                    tokenization_counter = 0
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º "–æ–∫–Ω–æ" –≤–æ–∫—Ä—É–≥ –ø–æ–∑–∏—Ü–∏–∏ context_size
                    context_window_end_pos = len(buffer) - context_size - 1
                    context_window_start_pos = context_window_end_pos - context_size_look_overhead
                    if context_window_start_pos < 0:
                        context_window_start_pos = 0

                    # –í—ã–∑—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    sentences = _tokenize_sentences(buffer, tokenize_sentences, 
                                                  poetic_mode, preserve_line_breaks)

                    if debug:
                        print(f"\033[36mbuffer: \"{buffer}\"\033[0m")
                        print(f"\033[36mDelimiter: '{delimiter_char}' at pos {len(buffer)-context_size}\033[0m")
                        print(f"\033[36mSentences found: {len(sentences)}\033[0m")

                    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
                    if len(sentences) > 1:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –≤—Å–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
                        total_length_except_last = sum(len(sentence) for sentence in sentences[:-1])
                        
                        if total_length_except_last >= minimum_sentence_length:
                            for sentence in sentences[:-1]:
                                # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω–æ–µ
                                if len(sentence) >= minimum_sentence_length // 2:
                                    yield_text = _clean_text(sentence, cleanup_text_emojis)
                                    if debug:
                                        print(f"\033[36mDebug: Yielding validated sentence: \"{yield_text}\"\033[0m")
                                    yield yield_text
                                    word_count = 0

                            if quick_yield_for_all_sentences:
                                is_first_sentence = True

                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –∫–æ–Ω—Ü–µ –±—É—Ñ–µ—Ä–∞ (–≤–∞–∂–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ)
                            ends_with_space = buffer.endswith(" ")
                            buffer = sentences[-1]
                            if ends_with_space:
                                buffer += " "

                            # Reset counters
                            last_delimiter_position = -1 
                            word_count = 0
                            fragment_count += 1
                            chars_since_last_strong_delim = 0

    # --- –í—ã–¥–∞—á–∞ –æ—Å—Ç–∞—Ç–∫–∞ –±—É—Ñ–µ—Ä–∞ –≤ –∫–æ–Ω—Ü–µ ---
    if buffer:
        sentences = _tokenize_sentences(buffer, tokenize_sentences, 
                                      poetic_mode, preserve_line_breaks)
        sentence_buffer = ""

        for sentence in sentences:
            sentence_buffer += sentence
            if len(sentence_buffer) < minimum_sentence_length:
                sentence_buffer += " "
                continue

            if debug:
                print(f"\033[36mDebug: Yielding final sentence(s): \"{sentence_buffer}\"\033[0m")
            yield_text = _clean_text(sentence_buffer, cleanup_text_emojis)
            yield yield_text
            sentence_buffer = ""

        if sentence_buffer:
            yield_text = _clean_text(sentence_buffer, cleanup_text_emojis)
            if debug:
                print(f"\033[36mDebug: Yielding remaining text: \"{yield_text}\"\033[0m")
            yield yield_text


# --- –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ ---

def _await_sync(f: Awaitable[str]) -> str:
    gen = f.__await__()
    try:
        next(gen)
        raise RuntimeError(f"{f} failed to be synchronous")
    except StopIteration as e:
        return e.value


def _async_iter_to_sync(f: AsyncIterator[str]) -> Iterator[str]:
    try:
        while True:
            yield _await_sync(f.__anext__())
    except StopAsyncIteration:
        return


P = ParamSpec("P")


def _dowrap(
    f: Callable[Concatenate[AsyncIterable[str], P], AsyncIterator[str]]
) -> Callable[Concatenate[Iterable[str], P], Iterator[str]]:
    @functools.wraps(f)
    def inner(generator: Iterable[str], *args: P.args, **kwargs: P.kwargs):
        # –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º cleanup_text_links –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω
        kwargs.pop('cleanup_text_links', None)
        async def gen_wrap():
            for x in generator:
                yield x

        return _async_iter_to_sync(f(gen_wrap(), *args, **kwargs))

    return inner


generate_sentences = _dowrap(generate_sentences_async)
generate_sentences.__name__ = "generate_sentences"
generate_sentences.__qualname__ = "generate_sentences"
